{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6284ad5d-c4df-484f-bd77-e146270a07ec",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4932bcdf-7ab3-4449-98a5-a12f1c4a9036",
   "metadata": {},
   "source": [
    "# Master Prompt Engineering and LangChain PromptTemplates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd1b598-ffe9-4fad-b89c-b11bdcea86ab",
   "metadata": {},
   "source": [
    "Estimated time needed: **45** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c108786a-2854-4ca9-b646-99f59cc2a410",
   "metadata": {},
   "source": [
    "## Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7667aec-3731-4072-8f87-c463772d5602",
   "metadata": {},
   "source": [
    "You're stepping into the world of prompt engineering, where each command you craft has the power to guide intelligent LLM systems toward specific outcomes. In this tutorial, you will explore the foundational aspects of prompt engineering, dive into advanced in-context learning techniques such as few-shot and self-consistent learning, and learn how to effectively use tools like LangChain to create prompt templates.\n",
    "\n",
    "You'll start by learning the basics—how to formulate prompts that communicate effectively with AI. From there, you'll explore how LangChain prompt templates can simplify and enhance this process, making it more structured and efficient.\n",
    "\n",
    "As you progress, you'll learn to apply these skills in practical scenarios, creating sophisticated applications like QA bots and text summarization tools. By using LangChain prompt templates, you'll see firsthand how structured prompting can streamline the development of these applications, transforming complex requirements into clear, concise tasks for AI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7f8f3d-0cf6-4d8f-a5f7-4fcf9c2518e0",
   "metadata": {},
   "source": [
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/ai8G4tOU4mksEYfv5wsghA/prompt%20engineering.png\" width=\"50%\" alt=\"indexing\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc85c3b-f2f1-42e3-a5e8-6dfdc9df7ace",
   "metadata": {},
   "source": [
    "By the end of this tutorial, you'll not only master various prompt engineering techniques but also gain hands-on experience applying these techniques to real-world problems, ensuring you're well-prepared to harness the full potential of AI in diverse settings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1341934f-221a-4675-998a-849e1145ff9b",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Install-required-libraries\">Install required libraries</a></li>\n",
    "            <li><a href=\"#Import-required-libraries\">Import required libraries</a></li>\n",
    "            <li><a href=\"#Set-up-the-LLM\">Set up the LLM</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Prompt-engineering\">Prompt engineering</a>\n",
    "        <ol>\n",
    "            <li>\n",
    "                <a href=\"#Basic-prompt\">Basic prompt</a>\n",
    "                <ol>\n",
    "                    <li><a href=\"#Exercise-1\">Exercise 1</a></li>\n",
    "                </ol>\n",
    "            </li>\n",
    "            <li>\n",
    "                <a href=\"#Zero-shot-prompt\">Zero-shot prompt</a>\n",
    "                <ol>\n",
    "                    <li><a href=\"#Exercise-2\">Exercise 2</a></li>\n",
    "                </ol>\n",
    "            </li>\n",
    "            <li>\n",
    "                <a href=\"#One-shot-prompt\">One-shot prompt</a>\n",
    "                <ol>\n",
    "                    <li><a href=\"#Exercise-3\">Exercise 3</a></li>\n",
    "                </ol>\n",
    "            </li>\n",
    "            <li><a href=\"#Few-shot-prompt\">Few-shot prompt</a></li>\n",
    "            <li>\n",
    "                <a href=\"#Chain-of-thought-(CoT)-prompt\">Chain-of-thought (CoT) prompt</a>\n",
    "                <ol>\n",
    "                    <li><a href=\"#Exercise-4\">Exercise 4</a></li>\n",
    "                </ol>\n",
    "            </li>\n",
    "            <li><a href=\"#Self-consistency\">Self-consistency</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Applications-of-prompting-in-different-use-cases\">Applications of prompting in different use cases</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Introduction-to-LangChain\">Introduction to LangChain</a></li>\n",
    "            <li><a href=\"#Prompt-template\">Prompt template</a></li>\n",
    "            <li><a href=\"#Text-summarization\">Text summarization</a></li>\n",
    "            <li><a href=\"#Question-answering\">Question answering</a></li>\n",
    "            <li><a href=\"#Text-classification\">Text classification</a></li>\n",
    "            <li><a href=\"#Code-generation\">Code generation</a></li>\n",
    "            <li><a href=\"#Role-playing\">Role playing</a></li>\n",
    "            <li><a href=\"#Exercise-5\">Exercise 5</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Conclusion\">Conclusion</a></li>\n",
    "    <li><a href=\"#Authors\">Authors</a></li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee7b7f5-8d8a-45fa-bb96-1039c028c48f",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab, you will be able to:\n",
    "\n",
    "- **Understand the basics of prompt engineering**: Gain a solid foundation in how to effectively communicate with LLM using prompts, setting the stage for more advanced techniques.\n",
    "\n",
    "- **Master advanced prompt techniques**: Learn and apply advanced prompt engineering methods such as few-shot and self-consistent learning to optimize the LLM's response.\n",
    "\n",
    "- **Utilize LangChain prompt templates**: Become proficient in using LangChain's prompt template to structure and optimize your interactions with LLMs.\n",
    "\n",
    "- **Develop practical LLM agents**: Acquire the skills to create and implement agents such as QA bots and text summarization tools using the LangChain prompt template, translating theoretical knowledge into practical solutions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61cf77e-86e3-4ea9-b886-eda82e237cfb",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee6e22c-d9e8-4630-9251-9f53b27ba337",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b02fe1b-f96c-4994-b538-b73032bd49a5",
   "metadata": {},
   "source": [
    "For this lab, you will use the following libraries:\n",
    "\n",
    "*   [`ibm-watsonx-ai`](https://ibm.github.io/watson-machine-learning-sdk/index.html): Enables the use of LLMs from IBM's watsonx.ai.\n",
    "*   [`langchain`](https://www.langchain.com/): Provides various chain and prompt functions from LangChain.\n",
    "*   [`langchain-ibm`](https://python.langchain.com/v0.1/docs/integrations/llms/ibm_watsonx/): Facilitates integration between LangChain and IBM watsonx.ai.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bf2588-4b0a-4679-8c7a-849513257d84",
   "metadata": {},
   "source": [
    "### Install required libraries\n",
    "\n",
    "The following required libraries are __not__ preinstalled in the Skills Network Labs environment. __You must run the following cell__ to install them:\n",
    "\n",
    "**Note:** The version has been pinned to ensure compatibility. It's recommended that you do the same. While future updates may be available, the pinned version ensures that the library continues to support this lab.\n",
    "\n",
    "This might take approximately 1-2 minutes.\n",
    "\n",
    "`%%capture` has been used to capture the installation, so you won’t see the process. However, once the installation is complete, a number will appear next to the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "232fe121-074e-41fa-91c9-f83b9bcaf23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install \"ibm-watsonx-ai==1.0.8\" --user\n",
    "# !pip install \"langchain==0.2.11\" --user\n",
    "# !pip install \"langchain-ibm==0.1.7\" --user\n",
    "# !pip install \"langchain-core==0.2.43\" --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136fe63a-051b-4abe-a63b-bfa10c78ba1e",
   "metadata": {},
   "source": [
    "### Import required libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fb79e8-494e-43ff-84d6-88fd826429d6",
   "metadata": {},
   "source": [
    "After installing the libraries, you must restart your kernel by clicking the **Restart the kernel** icon.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/6DtO5_X9SAK4tYJCnsre2w/restart-kernel.png\" width=\"80%\" alt=\"Restart kernel\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d517ce-9192-4571-8b17-274fea08c7e0",
   "metadata": {},
   "source": [
    "_It is recommended that you import all required libraries in one place (here):_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e289616-7a6d-4db4-8878-92f1ff5ad66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# IBM WatsonX imports\n",
    "from ibm_watsonx_ai.foundation_models import Model\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
    "\n",
    "# from langchain_ibm import WatsonxLLM\n",
    "from langchain_community.chat_models import ChatZhipuAI\n",
    "\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableSequence\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain.chains import LLMChain  # Still using this for backward compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c403148-b720-4ad2-ac1c-a780d94bee7b",
   "metadata": {},
   "source": [
    "### Set up the LLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d850bb0f-b47c-4bf6-85f0-128df330a0e3",
   "metadata": {},
   "source": [
    "In this section, you will build an LLM using IBM watsonx.ai. The following code initializes a Granite model on IBM's watsonx.ai platform and wraps it into a function that allows for repeat use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99731b27-788a-490f-a83d-1cf71777a418",
   "metadata": {},
   "source": [
    "Some key parameters are explained here:\n",
    "- `model_id` specifies which model you want to use. There are various model options available; refer to the [Foundation Models](https://ibm.github.io/watsonx-ai-python-sdk/foundation_models.html) documentation for more options. In this tutorial, you'll use the `granite-3-2-8b-instruct` model.\n",
    "- `parameters` define the model's configuration. Set five commonly used parameters for this tutorial. To explore additional commonly used parameters, you can run the code `GenParams().get_example_values()`. If no custom parameters are passed to the function, the model will use `default_params`.\n",
    "- `credentials` and `project_id` are required to successfully run LLMs from watsonx.ai. (Leave `credentials` and `project_id` as they are so you don't need to create your own keys to run models.) This ensures you can run the model inside this lab environment. However, if you want to run the model locally, refer to this [tutorial](https://medium.com/the-power-of-ai/ibm-watsonx-ai-the-interface-and-api-e8e1c7227358) for creating your own keys.\n",
    "- `WatsonxLLM()` is used to create an instance of the LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5841e992-c482-491c-9f7f-add85da4bd24",
   "metadata": {},
   "source": [
    "## API Disclaimer\n",
    "This lab uses LLMs provided by **Watsonx.ai**. This environment has been configured to allow LLM use without API keys so you can prompt them for **free (with limitations)**. With that in mind, if you wish to run this notebook **locally outside** of Skills Network's JupyterLab environment, you will have to **configure your own API keys**. Please note that using your own API keys means that you will incur personal charges.\n",
    "\n",
    "### Running Locally\n",
    "If you are running this lab locally, you will need to configure your own API keys. This lab uses the `WatsonxLLM` module from `IBM`, we'll need to initialize the `granite_llm` with credentials to use the LLM locally. Fill out the commented out `api_key` field in the cell below and pass in the credentials if you are running locally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc20c5ef-9b02-4e15-90e2-05cafe1c41c8",
   "metadata": {},
   "source": [
    "Run the following code to initialize the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac21b576-4e8c-4b10-ba4b-0d31ef71ca98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def llm_model(prompt_txt, params=None):\n",
    "    \n",
    "    # model_id = \"ibm/granite-3-2-8b-instruct\"\n",
    "    model_id = \"glm-4-plus\"\n",
    "\n",
    "    default_params = {\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"min_new_tokens\": 0,\n",
    "        \"temperature\": 0.5,\n",
    "        \"top_p\": 0.2,\n",
    "        \"top_k\": 1\n",
    "    }\n",
    "\n",
    "    if params:\n",
    "        default_params.update(params)\n",
    "\n",
    "    # Set up credentials for WatsonxLLM\n",
    "    # url = \"https://us-south.ml.cloud.ibm.com\"\n",
    "    # api_key = \"your api key here\"\n",
    "    # project_id = \"skills-network\"\n",
    "\n",
    "    credentials = {\n",
    "        # \"url\": url,\n",
    "        \"api_key\":os.getenv('ZHIPUAI_API_KEY')\n",
    "        # uncomment the field above and replace the api_key with your actual Watsonx API key\n",
    "    }\n",
    "    \n",
    "    # # Create LLM directly\n",
    "    # granite_llm = WatsonxLLM(\n",
    "    #     model_id=model_id,\n",
    "    #     credentials=credentials,\n",
    "    #     project_id=project_id,\n",
    "    #     params=default_params\n",
    "    # )\n",
    "\n",
    "    granite_llm = ChatZhipuAI(\n",
    "        model_id=model_id,\n",
    "        credentials=credentials,\n",
    "        params=default_params\n",
    "    )\n",
    "\n",
    "    response = granite_llm.invoke(prompt_txt)\n",
    "    return response "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182c1142-2227-4018-b3b3-c439b4fa0ee1",
   "metadata": {},
   "source": [
    "Let's run the following code to see some other commonly used parameters and their default values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf687587-6297-459d-9de1-909d0ec01b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decoding_method': 'sample',\n",
       " 'length_penalty': {'decay_factor': 2.5, 'start_index': 5},\n",
       " 'temperature': 0.5,\n",
       " 'top_p': 0.2,\n",
       " 'top_k': 1,\n",
       " 'random_seed': 33,\n",
       " 'repetition_penalty': 2,\n",
       " 'min_new_tokens': 50,\n",
       " 'max_new_tokens': 200,\n",
       " 'stop_sequences': ['fail'],\n",
       " ' time_limit': 600000,\n",
       " 'truncate_input_tokens': 200,\n",
       " 'prompt_variables': {'object': 'brain'},\n",
       " 'return_options': {'input_text': True,\n",
       "  'generated_tokens': True,\n",
       "  'input_tokens': True,\n",
       "  'token_logprobs': True,\n",
       "  'token_ranks': False,\n",
       "  'top_n_tokens': False}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GenParams().get_example_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8d7134-5248-4b05-91ac-a01f9e1999f5",
   "metadata": {},
   "source": [
    "## Prompt engineering\n",
    "\n",
    "[Prompt engineering](https://www.ibm.com/think/topics/prompt-engineering?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-In-Context+Learning+and+Prompt+Templates-v3-GenAIcourse_1741386184) is the art and science of crafting effective inputs for large language models to generate desired outputs. As language models have evolved in capability and size, so too has the importance of how we communicate with them. Prompt engineering involves strategically designing text prompts that guide an AI model's responses toward specific goals, formats, or reasoning patterns.\n",
    "\n",
    "**[In-context learning](https://research.ibm.com/blog/demystifying-in-context-learning-in-large-language-model?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-In-Context+Learning+and+Prompt+Templates-v3-GenAIcourse_1741386184)** represents one of the most fascinating capabilities of modern large language models. It is a model's ability to \"learn\" from examples provided directly within the prompt itself, without any updates to its underlying parameters or weights. This capability allows models to adapt to new tasks or domains simply by demonstrating what success looks like through examples.\n",
    "\n",
    "By combining the principles of prompt engineering with the power of in-context learning, developers can guide language models to perform a remarkably diverse range of tasks with unprecedented flexibility and efficiency. Let's now explore these methods in detail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5ebb6c-121d-4030-adcc-5653a653aa4d",
   "metadata": {},
   "source": [
    "### Basic prompt\n",
    "\n",
    "A **basic prompt** is the simplest form of prompting, where you provide a short text or phrase to the model without any special formatting or instructions. The model generates a continuation based on patterns it has learned during training. Basic prompts are useful for exploring the model's capabilities and understanding how it naturally responds to minimal input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8692d225-f496-46c8-940f-5e39ce745d3c",
   "metadata": {},
   "source": [
    "In this example, let's introduce a basic prompt that uses specific parameters to guide the language model's response. You'll then define a simple prompt and retrieve the model's response.\n",
    "\n",
    "The prompt used is \"The wind is.\" Let the model generate itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b7a2113-ec3c-4ed1-8be9-b056ae08fcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: The wind is\n",
      "\n",
      "response : content='The wind is a natural phenomenon that is the movement of air or other gases relative to the surface of a planet. It occurs as a result of the differences in atmospheric pressure and is influenced by factors such as temperature, humidity, and the rotation of the Earth. The wind plays a crucial role in shaping weather patterns, can be harnessed as a renewable energy source, and has a significant impact on various ecosystems. If you have a more specific question or context in mind, feel free to share, and I can provide a more detailed response.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 111, 'prompt_tokens': 8, 'total_tokens': 119}, 'model_name': 'glm-4', 'finish_reason': 'stop'} id='run--df655935-06f8-4ebf-ae85-2b2f5b332b89-0'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"min_new_tokens\": 10,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 0.2,\n",
    "    \"top_k\": 1\n",
    "}\n",
    "\n",
    "prompt = \"The wind is\"\n",
    "\n",
    "# Getting a reponse from the model with the provided prompt and new parameters\n",
    "response = llm_model(prompt, params)\n",
    "print(f\"prompt: {prompt}\\n\")\n",
    "print(f\"response : {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ab14a7-e55e-419a-a9f3-26766e16c568",
   "metadata": {},
   "source": [
    "As you can see from the response, the model continues generating content based on the initial prompt, \"The wind is.\" You might notice that the response appears truncated or incomplete. This is because you have set the `max_new_tokens,` which restricts the number of tokens the model can generate.\n",
    "\n",
    "Try adjusting the parameters and observe how the response changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dce578-7c51-4d0e-88f7-1f57f4d18296",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Experiment with different basic prompts by changing the input phrase. Try these prompts and compare the different responses:\n",
    "1. \"The future of artificial intelligence is\"\n",
    "2. \"Once upon a time in a distant galaxy\"\n",
    "3. \"The benefits of sustainable energy include\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcfa8d55-c6bf-4bbc-9d0d-71f9d18f65c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: The future of artificial intelligence is\n",
      "\n",
      "response : content=\"The future of artificial intelligence (AI) is a topic of great interest and speculation. The development of AI is poised to significantly transform various aspects of our lives, including work, healthcare, education, entertainment, and transportation, among others. Here are some potential directions for the future of AI:\\n\\n1. **Increased Automation**: AI is expected to continue driving automation in industries, leading to increased efficiency and productivity, but also raising questions about job displacement and the need for reskilling workers.\\n\\n2. **Enhanced Decision-Making**: With advancements in machine learning and predictive analytics, AI will play a crucial role in helping humans make better-informed decisions in complex situations.\\n\\n3. **Healthcare Advances**: AI has the potential to revolutionize healthcare by assisting in diagnostics, personalized medicine, drug discovery, and the management of patient care.\\n\\n4. **Transportation Revolution**: Self-driving cars and autonomous vehicles are on the horizon, promising to transform transportation and reduce accidents, but also presenting new challenges in terms of regulation, ethics, and infrastructure.\\n\\n5. **AI in Education**: Personalized learning experiences tailored to individual student needs and learning styles are expected to become more common, with AI acting as tutors and assistants.\\n\\n6. **Ethical and Social Implications**: As AI becomes more sophisticated, there will be a greater focus on ethical considerations, including privacy concerns, algorithmic biases, and the need for guidelines on AI's use in areas such as law enforcement and national security.\\n\\n7. **Collaboration with Humans**: The future of AI will likely involve more seamless collaboration with humans, with AI systems acting as partners rather than replacements, enhancing human capabilities in creative and unforeseen ways.\\n\\n8. **AI in Space Exploration**: AI and robotics could extend human reach into space, enabling the exploration of the solar system and beyond, where human presence is not feasible or too risky.\\n\\n9. **Economic Disruptions**: AI has the potential to shift economic paradigms, affecting the distribution of wealth and job opportunities. There may be a growing need for policies to address these economic disruptions.\\n\\n10. **Singularity and Beyond**: There is ongoing debate about the possibility of a technological singularity, a hypothetical future point where AI surpasses human intelligence and leads to unpredictable and rapid changes. While this remains a theoretical concept, it influences discussions about the long-term future of AI.\\n\\nIt's important to note that the future of AI will be shaped not only by technological advancements but also by social, economic, ethical, and political considerations. The responsible development and deployment of AI will be crucial to ensure that the benefits of this technology are realized while minimizing potential negative consequences.\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 532, 'prompt_tokens': 11, 'total_tokens': 543}, 'model_name': 'glm-4', 'finish_reason': 'stop'} id='run--55b5b26c-2fd9-4b5b-9d87-470fc03f6bfb-0'\n",
      "\n",
      "prompt: Once upon a time in a distant galaxy\n",
      "\n",
      "response : content=\"Once upon a time in a distant galaxy, where the stars shone with a brilliance unknown to our own universe, there existed a vast and magnificent civilization. This civilization was home to countless alien races, each more bizarre and wondrous than the last. They had harnessed the power of celestial forces, and their technology was far beyond anything we could comprehend.\\n\\nIn this galaxy, there was a planet named Aetheria, a world of enchantment and mystery. It was here that the most advanced race, the Ethereals, had built their grand capital. The Ethereals were beings of pure energy, able to manipulate the very fabric of reality. They were governed by a council of wise and ancient entities who had maintained peace and prosperity throughout the galaxy for eons.\\n\\nHowever, trouble was brewing in the shadows. An ancient force, known only as the Dark Nexus, had awakened from its slumber deep within the galaxy's core. It sought to consume all life and plunge the galaxy into eternal darkness. The Ethereals, realizing the impending doom, banded together with the other races to form an alliance against this common enemy.\\n\\nLed by the brave warrior princess, Luma, and her trusty companion, a wise old android named MX-7, the alliance embarked on a perilous journey to gather the fabled Elements of Light. These powerful artifacts were hidden across the galaxy and were the only hope of stopping the Dark Nexus.\\n\\nAs their adventure unfolded, they faced countless challenges and encountered fierce enemies. From the swampy world of Vorgoth, where they battled monstrous creatures, to the floating cities of the Skyborn, where they raced against time to prevent the destruction of a peaceful civilization, Luma and MX-7 discovered the true meaning of courage, friendship, and sacrifice.\\n\\nIn their final confrontation with the Dark Nexus, the alliance unleashed the combined power of the Elements of Light, leading to a battle of epic proportions. The galaxy itself seemed to hold its breath as the fate of all existence hung in the balance. With a final, resounding cry, Luma and her allies vanquished the Dark Nexus, restoring harmony to the galaxy.\\n\\nAnd so, in that distant galaxy, the story of Luma and her valiant allies became legend, passed down through the ages as a reminder of the strength and resilience of the spirit. Their victory served as a beacon of hope, ensuring that light would always triumph over darkness, and that the galaxy would continue to flourish for eons to come.\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 507, 'prompt_tokens': 13, 'total_tokens': 520}, 'model_name': 'glm-4', 'finish_reason': 'stop'} id='run--f8719413-1075-43cf-87a5-cb62d130fd03-0'\n",
      "\n",
      "prompt: The benefits of sustainable energy include\n",
      "\n",
      "response : content='Sustainable energy offers a wide array of benefits, which are crucial for the environment, the economy, and society as a whole. Here are some key advantages:\\n\\n1. **Reduced Greenhouse Gas Emissions**: Sustainable energy sources produce little to no greenhouse gas emissions during operation, helping to mitigate climate change.\\n\\n2. **Improved Air Quality**: Unlike fossil fuels, which release pollutants into the air, sustainable energy sources such as wind and solar do not produce harmful emissions that contribute to smog, acid rain, or respiratory problems.\\n\\n3. **Water Conservation**: Many sustainable energy technologies, such as solar and wind power, require minimal water for operation, in contrast to coal and nuclear power plants that use large amounts of water for cooling and other processes.\\n\\n4. **Energy Security**: Renewable energy diversifies energy supply and reduces dependence on imported fuels, enhancing energy security and reducing vulnerability to price fluctuations and political tensions associated with fossil fuel markets.\\n\\n5. **Long-term Cost Savings**: While the initial investment for some renewable energy technologies can be higher, the operational costs are often much lower. Once installed, renewable energy systems can provide energy at little to no cost, leading to long-term savings.\\n\\n6. **Job Creation**: The renewable energy sector creates employment opportunities, both in the manufacturing and installation of technologies and in the maintenance and operation of energy facilities.\\n\\n7. **Local Economic Development**: Renewable energy projects can stimulate local economies by providing income to landowners, tax revenue to local governments, and business opportunities for local supply chains.\\n\\n8. **Distributed Generation**: Many sustainable energy systems can be set up on a small scale, allowing for distributed generation, which can reduce transmission losses and increase grid resilience.\\n\\n9. **Sustainability**: Renewable energy sources are, by definition, sustainable over the long term because they rely on infinite or nearly infinite natural resources like sunlight, wind, and water.\\n\\n10. **Innovation and Technological Advancement**: The push for sustainable energy has led to significant technological innovation, which can have spillover effects into other sectors of the economy.\\n\\n11. **Social Benefits**: Renewable energy can bring electricity to remote or underdeveloped areas, improving quality of life and providing educational and economic opportunities.\\n\\n12. **Wildlife and Habitat Preservation**: Some sustainable energy sources can be sited in a manner that avoids sensitive ecological areas, and they generally cause less harm to wildlife and ecosystems than fossil fuel extraction and combustion.\\n\\n13. **Lower Risk of Energy Price Volatility**: Since renewable energy sources like solar and wind have no fuel cost, they can offer a hedge against the price volatility of fossil fuels.\\n\\n14. **Compatibility with a Circular Economy**: Renewable energy technologies can be designed for longevity, with materials being recycled or repurposed at the end of their useful life, supporting a circular economy.\\n\\n15. **Public Health**: By reducing air and water pollution, sustainable energy can significantly improve public health by lowering the incidence of respiratory and cardiovascular diseases.\\n\\nThese benefits underscore the importance of transitioning to a sustainable energy economy as part of a broader strategy to address environmental challenges, economic growth, and social well-being.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 625, 'prompt_tokens': 11, 'total_tokens': 636}, 'model_name': 'glm-4', 'finish_reason': 'stop'} id='run--3bc663d2-8b17-425e-ab62-5afe3d779a04-0'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Your code here\n",
    "params = {\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"min_new_tokens\": 10,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 0.2,\n",
    "    \"top_k\": 1\n",
    "}\n",
    "\n",
    "prompts = [\n",
    "    \"The future of artificial intelligence is\",\n",
    "    \"Once upon a time in a distant galaxy\",\n",
    "    \"The benefits of sustainable energy include\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    response = llm_model(prompt, params)\n",
    "    print(f\"prompt: {prompt}\\n\")\n",
    "    print(f\"response : {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5274d473-933c-4c84-8c27-135ecc263011",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for hints</summary>\n",
    "\n",
    "```python\n",
    "params = {\n",
    "    \"max_new_tokens\": 128, # Try 256 or 512 for more detailed answers\n",
    "    \"min_new_tokens\": 10, # Increase to 25-50 if you want more substantial answers\n",
    "    \"temperature\": 0.5, # Controls randomness in generation (0.0-1.0)\n",
    "                       # Lower (0.1-0.3): More focused, consistent, factual responses\n",
    "                      # Higher (0.7-1.0): More creative, diverse, unpredictable outputs\n",
    "    \"top_p\": 0.2, # Nucleus sampling - considers only highest probability tokens\n",
    "                       # Lower values (0.1-0.3): More conservative, focused text\n",
    "                       # Higher values (0.7-0.9): More diverse vocabulary and ideas\n",
    "    \"top_k\": 1 # Limits token selection to top k most likely tokens\n",
    "                       # 1 = greedy decoding (always picks most likely token)\n",
    "                       # Try 40-50 for more varied outputs\n",
    "}\n",
    "\n",
    "# Compare responses to different prompts\n",
    "prompts = [\n",
    "    \"The future of artificial intelligence is\",\n",
    "    \"Once upon a time in a distant galaxy\",\n",
    "    \"The benefits of sustainable energy include\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    response = llm_model(prompt, params)\n",
    "    print(f\"prompt: {prompt}\\n\")\n",
    "    print(f\"response : {response}\\n\")\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fee48d-2d47-4d77-a7d7-d116c01940c7",
   "metadata": {},
   "source": [
    "### Zero-shot prompt\n",
    "\n",
    "[**Zero-shot prompting**](https://www.ibm.com/think/topics/zero-shot-prompting?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-In-Context+Learning+and+Prompt+Templates-v3-GenAIcourse_1741386184) is a technique where the model performs a task without any examples or prior specific training on that task. This approach tests the model's ability to understand instructions and apply its knowledge to a new context without demonstration. Zero-shot prompts typically include clear instructions about what the model should do, allowing it to leverage its pre-trained knowledge effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82de7d41-2b02-41fe-927a-69dafec0e0f9",
   "metadata": {},
   "source": [
    "---\n",
    "Here is an example of a zero-shot prompt:\n",
    "\n",
    "Zero-shot learning is crucial for testing a model's ability to apply its pre-trained knowledge to new, unseen tasks without additional training. This capability is valuable for gauging the model's generalization skills.\n",
    "\n",
    "In this example, let's demonstrate a zero-shot learning scenario using a prompt that asks the model to classify a statement without any prior specific training on similar tasks. The prompt requests the model to assess the truthfulness of the statement: \"The Eiffel Tower is located in Berlin.\" After defining the prompt, you'll execute it with default parameters and print the response.\n",
    "\n",
    "This approach helps you understand how well the model can handle direct questions based on its underlying knowledge and reasoning abilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0915f58-51e2-4103-9eb9-d2d2aa90d802",
   "metadata": {},
   "source": [
    "Try running the prompt to see the model's capacity to correctly analyze and respond to factual inaccuracies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e91f9826-a27c-42cb-afea-c340c1bb7176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: Classify the following statement as true or false: \n",
      "            'The Eiffel Tower is located in Berlin.'\n",
      "\n",
      "            Answer:\n",
      "\n",
      "\n",
      "response : content='False. The Eiffel Tower is located in Paris, France, not Berlin.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 31, 'total_tokens': 50}, 'model_name': 'glm-4', 'finish_reason': 'stop'} id='run--e2cccb16-8090-4b50-87c4-4d0c4804ff2e-0'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Classify the following statement as true or false: \n",
    "            'The Eiffel Tower is located in Berlin.'\n",
    "\n",
    "            Answer:\n",
    "\"\"\"\n",
    "response = llm_model(prompt, params)\n",
    "print(f\"prompt: {prompt}\\n\")\n",
    "print(f\"response : {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dda38cb-5747-457d-8a2b-e5d311bf4b72",
   "metadata": {},
   "source": [
    "The model responds with the 'False' answer, which is correct. It also gives the reason for the answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7e4e59-93a5-408f-909a-0c70a1168400",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Create zero-shot prompts for the following tasks:\n",
    "1. Write a prompt that asks the model to classify a movie review as positive or negative.\n",
    "2. Create a prompt that instructs the model to summarize a paragraph about climate change.\n",
    "3. Design a prompt that asks the model to translate an English phrase to Spanish without showing any examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df728ce5-f829-4a08-90f2-650c892193a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MOVIE_REVIEW RESPONSE ===\n",
      "content='Positive' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 64, 'total_tokens': 67}, 'model_name': 'glm-4', 'finish_reason': 'stop'} id='run--60dbccb2-fef1-4838-b7b4-3ee35cf26404-0'\n",
      "\n",
      "=== CLIMATE_CHANGE RESPONSE ===\n",
      "content='Climate change encompasses long-term alterations in temperature and weather, predominantly caused by human activities such as burning fossil fuels. This leads to increased droughts, storms, and heat waves, along with rising sea levels, melting glaciers, and warmer oceans, posing risks to biodiversity, agriculture, and human welfare.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 61, 'prompt_tokens': 123, 'total_tokens': 184}, 'model_name': 'glm-4', 'finish_reason': 'stop'} id='run--0805a14e-8625-404f-b809-42de80ed5050-0'\n",
      "\n",
      "=== TRANSLATION RESPONSE ===\n",
      "content='Spanish: \"Me gustaría pedir un café con leche y dos azúcares, por favor.\"' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 33, 'total_tokens': 58}, 'model_name': 'glm-4', 'finish_reason': 'stop'} id='run--cff71a59-5967-4b72-a32e-d9616862d4a6-0'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Starter code: provide your solutions in the TODO parts\n",
    "\n",
    "# 1. Prompt for Movie Review Classification\n",
    "movie_review_prompt = \"\"\" \n",
    "Classify the following movie review as either \"positive\" or \"negative\":\n",
    "\n",
    "Review: \"An absolute masterpiece! The cinematography was breathtaking, and the performances were top-notch. \n",
    "I was completely captivated from start to finish. Highly recommend it to anyone who loves a good story!\"\n",
    "\n",
    "Classification:\n",
    "\"\"\"\n",
    "\n",
    "# 2. Prompt for Climate Change Paragraph Summarization\n",
    "climate_change_prompt = \"\"\"\n",
    "Summarize the following paragraph on climate change:\n",
    "\n",
    "Paragraph: \"\"Climate change refers to long-term shifts in temperatures and weather patterns. \n",
    "These shifts may be natural, but since the 1800s, human activities have been the main driver of climate change, primarily due to the burning of fossil fuels like coal, oil and gas, which produces heat-trapping gases. \n",
    "The consequences of climate change include more frequent and severe droughts, storms, and heat waves, rising sea levels, melting glaciers, and warming oceans which can directly impact biodiversity, agriculture, and human health.\"\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "# 3. Prompt for English to Spanish Translation\n",
    "translation_prompt = \"\"\" \n",
    "Translate the following English phrase into Spanish:\n",
    "\n",
    "English: \"I would like to order a coffee with milk and two sugars, please.\"\n",
    "\n",
    "Spanish:\n",
    "\"\"\"\n",
    "\n",
    "responses = {}\n",
    "responses[\"movie_review\"] = llm_model(movie_review_prompt)\n",
    "responses[\"climate_change\"] = llm_model(climate_change_prompt)\n",
    "responses[\"translation\"] = llm_model(translation_prompt)\n",
    "\n",
    "for prompt_type, response in responses.items():\n",
    "    print(f\"=== {prompt_type.upper()} RESPONSE ===\")\n",
    "    print(response)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bc304b-5520-4398-882b-55363d7cab41",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for hints</summary>\n",
    "\n",
    "```python\n",
    "# 1. Prompt for Movie Review Classification\n",
    "movie_review_prompt = \"\"\"\n",
    "Classify the following movie review as either 'positive' or 'negative'.\n",
    "\n",
    "Review: \"I was extremely disappointed by this film. The plot was predictable, the acting was wooden, and the special effects looked cheap. I can't recommend this to anyone.\"\n",
    "\n",
    "Classification:\n",
    "\"\"\"\n",
    "\n",
    "# 2. Prompt for Climate Change Paragraph Summarization\n",
    "climate_change_prompt = \"\"\"\n",
    "Summarize the following paragraph about climate change in no more than two sentences.\n",
    "\n",
    "Paragraph: \"Climate change refers to long-term shifts in temperatures and weather patterns. These shifts may be natural, but since the 1800s, human activities have been the main driver of climate change, primarily due to the burning of fossil fuels like coal, oil and gas, which produces heat-trapping gases. The consequences of climate change include more frequent and severe droughts, storms, and heat waves, rising sea levels, melting glaciers, and warming oceans which can directly impact biodiversity, agriculture, and human health.\"\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "# 3. Prompt for English to Spanish Translation\n",
    "translation_prompt = \"\"\"\n",
    "Translate the following English phrase into Spanish.\n",
    "\n",
    "English: \"I would like to order a coffee with milk and two sugars, please.\"\n",
    "\n",
    "Spanish:\n",
    "\"\"\"\n",
    "\n",
    "responses = {}\n",
    "responses[\"movie_review\"] = llm_model(movie_review_prompt)\n",
    "responses[\"climate_change\"] = llm_model(climate_change_prompt)\n",
    "responses[\"translation\"] = llm_model(translation_prompt)\n",
    "\n",
    "for prompt_type, response in responses.items():\n",
    "    print(f\"=== {prompt_type.upper()} RESPONSE ===\")\n",
    "    print(response)\n",
    "    print()\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8208f56-5b25-4b5c-b212-28ed8cea9a71",
   "metadata": {},
   "source": [
    "### One-shot prompt\n",
    "\n",
    "[**One-shot prompting**](https://www.ibm.com/think/topics/one-shot-prompting?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-In-Context+Learning+and+Prompt+Templates-v3-GenAIcourse_1741386184) provides the model with a **single** example of the task before asking it to perform a similar task. This technique gives the model a pattern to follow, improving its understanding of the desired output format and style. One-shot learning is particularly useful when you want to guide the model's response without extensive examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50a7e74-9150-4f00-a488-b84e58c2c3b9",
   "metadata": {},
   "source": [
    "Here is a one-shot learning example where the model is given a single example to help guide its translation from English to French.\n",
    "\n",
    "The prompt provides a sample translation pairing, \"How is the weather today?\" translated to \"Comment est le temps aujourd'hui?\" This example serves as a guide for the model to understand the task context and desired format. The model is then tasked with translating a new sentence, \"Where is the nearest supermarket?\" without further guidance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f80f236-de19-481a-898d-4cab90994b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: Here is an example of translating a sentence from English to French:\n",
      "\n",
      "            English: “How is the weather today?”\n",
      "            French: “Comment est le temps aujourd'hui?”\n",
      "\n",
      "            Now, translate the following sentence from English to French:\n",
      "\n",
      "            English: “Where is the nearest supermarket?”\n",
      "\n",
      "\n",
      "\n",
      "response : content='French: \"Où se trouve le supermarché le plus proche ?\"' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 62, 'total_tokens': 83}, 'model_name': 'glm-4', 'finish_reason': 'stop'} id='run--6c6721ce-8589-4b25-b08c-4666cabb8b0f-0'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_new_tokens\": 20,\n",
    "    \"temperature\": 0.1,\n",
    "}\n",
    "\n",
    "prompt = \"\"\"Here is an example of translating a sentence from English to French:\n",
    "\n",
    "            English: “How is the weather today?”\n",
    "            French: “Comment est le temps aujourd'hui?”\n",
    "            \n",
    "            Now, translate the following sentence from English to French:\n",
    "            \n",
    "            English: “Where is the nearest supermarket?”\n",
    "            \n",
    "\"\"\"\n",
    "response = llm_model(prompt, params)\n",
    "print(f\"prompt: {prompt}\\n\")\n",
    "print(f\"response : {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b6352e-da10-4b4b-aad2-9ecc393fdeb3",
   "metadata": {},
   "source": [
    "The model's response shows how it applies the structure and context provided by the initial example to translate the new sentence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd550a6-31e1-416b-bbf6-7b7d932bbe9f",
   "metadata": {},
   "source": [
    "Consider experimenting with different sentences or adjusting the parameters to see how these changes impact the model's translations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fe3956-6b34-4f14-a99c-d47cd44cef7a",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Develop one-shot prompts for these scenarios:\n",
    "1. Create a prompt with one example of a formal email, then ask the model to write another formal email on a different topic.\n",
    "2. Provide one example of converting a technical concept into a simple explanation, then ask the model to explain a different concept.\n",
    "3. Give one example of extracting keywords from a sentence, then ask the model to extract keywords from a new sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b524155c-1b7d-446f-95c3-f691e8007915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FORMAL_EMAIL RESPONSE ===\n",
      "content=\"Subject: Request for Application Deadline and Required Documents for Master's in Computer Science\\n\\nDear Admissions Office,\\n\\nI trust this message finds you well. I am writing to inquire about the upcoming application deadline and the required documents for the Master's program in Computer Science at your esteemed university.\\n\\nAs I am keen on applying to your program, I would appreciate if you could provide me with the following information:\\n\\n1. The deadline for submitting applications for the upcoming academic year.\\n2. A list of the required documents that must accompany the application.\\n3. Any specific prerequisites or qualifications that are expected from applicants.\\n\\nAdditionally, if there are any supplementary materials or recommendations that could enhance my application, I would be grateful if you could advise on those as well.\\n\\nCould you kindly provide this information at your earliest convenience? It would greatly assist me in preparing a comprehensive application packet.\\n\\nThank you for your time and assistance. I look forward to the possibility of joining your institution and contributing to its academic excellence.\\n\\nWarm regards,\\n\\n[Your Full Name]\\n[Your Contact Information]\\n[Your Current Education or Professional Background, if relevant]\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 222, 'prompt_tokens': 150, 'total_tokens': 372}, 'model_name': 'glm-4', 'finish_reason': 'stop'} id='run--c81c2326-1bda-4fb0-b79d-5a6542d739fb-0'\n",
      "\n",
      "=== TECHNICAL_CONCEPT RESPONSE ===\n",
      "content=\"Machine Learning is like teaching a computer to learn from examples, just like a child learns to recognize the shape of a cat by seeing many different cats. The computer uses these examples to understand patterns and make decisions or predictions. Think of it as giving the computer a set of flashcards and letting it figure out the rules on its own. Over time, the computer gets better and better at recognizing the right answers, even with new flashcards it hasn't seen before.\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 95, 'prompt_tokens': 117, 'total_tokens': 212}, 'model_name': 'glm-4', 'finish_reason': 'stop'} id='run--88049b18-b340-47dc-af36-22cf29bef648-0'\n",
      "\n",
      "=== KEYWORD_EXTRACTION RESPONSE ===\n",
      "content='Sustainable agriculture, biodiversity, soil health, water conservation, reducing chemical inputs.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 90, 'total_tokens': 108}, 'model_name': 'glm-4', 'finish_reason': 'stop'} id='run--6aee9a17-1e8e-4eac-be0a-8cd2be718e4b-0'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Starter code: provide your solutions in the TODO parts\n",
    "\n",
    "# 1. One-shot prompt for formal email writing\n",
    "formal_email_prompt = \"\"\"\n",
    "Here is an example of a formal email requesting information:\n",
    "\n",
    "Subject: Inquiry Regarding Product Specifications for Model XYZ-100\n",
    "\n",
    "Dear Customer Support Team,\n",
    "\n",
    "I hope this email finds you well. I am writing to request detailed specifications for your product Model XYZ-100. Specifically, I am interested in learning about its dimensions, power requirements, and compatibility with third-party accessories.\n",
    "\n",
    "Could you please provide this information at your earliest convenience? Additionally, I would appreciate any available documentation or user manuals that you could share.\n",
    "\n",
    "Thank you for your assistance in this matter.\n",
    "\n",
    "Sincerely,\n",
    "John Smith\n",
    "\n",
    "---\n",
    "\n",
    "Now, please write a formal email to a university admissions office requesting information about their application deadline and required documents for the Master's program in Computer Science:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 2. One-shot prompt for simplifying technical concepts\n",
    "technical_concept_prompt = \"\"\"\n",
    "Here is an example of explaining a technical concept in simple terms:\n",
    "\n",
    "Technical Concept: Blockchain\n",
    "Simple Explanation: A blockchain is like a digital notebook that many people have copies of. When someone writes a new entry in this notebook, everyone's copy gets updated. Once something is written, it can't be erased or changed, and everyone can see who wrote what. This makes it useful for recording important information that needs to be secure and trusted by everyone.\n",
    "\n",
    "---\n",
    "\n",
    "Now, please explain the following technical concept in simple terms:\n",
    "\n",
    "Technical Concept: Machine Learning\n",
    "Simple Explanation:\n",
    "\"\"\"\n",
    "\n",
    "# 3. One-shot prompt for keyword extraction\n",
    "keyword_extraction_prompt = \"\"\"\n",
    "Here is an example of extracting keywords from a sentence:\n",
    "\n",
    "Sentence: \"Cloud computing offers businesses flexibility, scalability, and cost-efficiency for their IT infrastructure needs.\"\n",
    "Keywords: cloud computing, flexibility, scalability, cost-efficiency, IT infrastructure\n",
    "\n",
    "---\n",
    "\n",
    "Now, please extract the main keywords from the following sentence:\n",
    "\n",
    "Sentence: \"Sustainable agriculture practices focus on biodiversity, soil health, water conservation, and reducing chemical inputs.\"\n",
    "Keywords:\n",
    "\"\"\"\n",
    "\n",
    "responses = {}\n",
    "responses[\"formal_email\"] = llm_model(formal_email_prompt)\n",
    "responses[\"technical_concept\"] = llm_model(technical_concept_prompt)\n",
    "responses[\"keyword_extraction\"] = llm_model(keyword_extraction_prompt)\n",
    "\n",
    "for prompt_type, response in responses.items():\n",
    "    print(f\"=== {prompt_type.upper()} RESPONSE ===\")\n",
    "    print(response)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f355f9d7-b1cb-42df-981e-8e88d9b3fbe3",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for hints</summary>\n",
    "\n",
    "```python\n",
    "# 1. One-shot prompt for formal email writing\n",
    "formal_email_prompt = \"\"\"\n",
    "Here is an example of a formal email requesting information:\n",
    "\n",
    "Subject: Inquiry Regarding Product Specifications for Model XYZ-100\n",
    "\n",
    "Dear Customer Support Team,\n",
    "\n",
    "I hope this email finds you well. I am writing to request detailed specifications for your product Model XYZ-100. Specifically, I am interested in learning about its dimensions, power requirements, and compatibility with third-party accessories.\n",
    "\n",
    "Could you please provide this information at your earliest convenience? Additionally, I would appreciate any available documentation or user manuals that you could share.\n",
    "\n",
    "Thank you for your assistance in this matter.\n",
    "\n",
    "Sincerely,\n",
    "John Smith\n",
    "\n",
    "---\n",
    "\n",
    "Now, please write a formal email to a university admissions office requesting information about their application deadline and required documents for the Master's program in Computer Science:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 2. One-shot prompt for simplifying technical concepts\n",
    "technical_concept_prompt = \"\"\"\n",
    "Here is an example of explaining a technical concept in simple terms:\n",
    "\n",
    "Technical Concept: Blockchain\n",
    "Simple Explanation: A blockchain is like a digital notebook that many people have copies of. When someone writes a new entry in this notebook, everyone's copy gets updated. Once something is written, it can't be erased or changed, and everyone can see who wrote what. This makes it useful for recording important information that needs to be secure and trusted by everyone.\n",
    "\n",
    "---\n",
    "\n",
    "Now, please explain the following technical concept in simple terms:\n",
    "\n",
    "Technical Concept: Machine Learning\n",
    "Simple Explanation:\n",
    "\"\"\"\n",
    "\n",
    "# 3. One-shot prompt for keyword extraction\n",
    "keyword_extraction_prompt = \"\"\"\n",
    "Here is an example of extracting keywords from a sentence:\n",
    "\n",
    "Sentence: \"Cloud computing offers businesses flexibility, scalability, and cost-efficiency for their IT infrastructure needs.\"\n",
    "Keywords: cloud computing, flexibility, scalability, cost-efficiency, IT infrastructure\n",
    "\n",
    "---\n",
    "\n",
    "Now, please extract the main keywords from the following sentence:\n",
    "\n",
    "Sentence: \"Sustainable agriculture practices focus on biodiversity, soil health, water conservation, and reducing chemical inputs.\"\n",
    "Keywords:\n",
    "\"\"\"\n",
    "\n",
    "responses = {}\n",
    "responses[\"formal_email\"] = llm_model(formal_email_prompt)\n",
    "responses[\"technical_concept\"] = llm_model(technical_concept_prompt)\n",
    "responses[\"keyword_extraction\"] = llm_model(keyword_extraction_prompt)\n",
    "\n",
    "for prompt_type, response in responses.items():\n",
    "    print(f\"=== {prompt_type.upper()} RESPONSE ===\")\n",
    "    print(response)\n",
    "    print()\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5e3edd-09e4-420b-82b9-67d787b41cba",
   "metadata": {},
   "source": [
    "### Few-shot prompt\n",
    "\n",
    "[**Few-shot prompting**](https://www.ibm.com/think/topics/few-shot-prompting?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-In-Context+Learning+and+Prompt+Templates-v3-GenAIcourse_1741386184) extends the one-shot approach by providing multiple examples (typically 2-5) before asking the model to perform the task. These examples establish a clearer pattern and context, helping the model better understand the expected output format, style, and reasoning. This technique is particularly effective for complex tasks where a single example might not convey all the nuances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672290a2-3bf6-49b9-8191-d70172e1071f",
   "metadata": {},
   "source": [
    "Here is an example of few-shot learning by classifying emotions from text statements. \n",
    "\n",
    "Let's provide the model with three examples, each labeled with an appropriate emotion—joy, frustration, and sadness—to establish a pattern or guideline on how to categorize emotions in statements.\n",
    "\n",
    "After presenting these examples, let's challenge the model with a new statement: \"That movie was so scary I had to cover my eyes.\" The task for the model is to classify the emotion expressed in this new statement based on the learning from the provided examples. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e569be-4905-413a-9860-8f04ef42b55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: Here are few examples of classifying emotions in statements:\n",
      "\n",
      "            Statement: 'I just won my first marathon!'\n",
      "            Emotion: Joy\n",
      "\n",
      "            Statement: 'I can't believe I lost my keys again.'\n",
      "            Emotion: Frustration\n",
      "\n",
      "            Statement: 'My best friend is moving to another country.'\n",
      "            Emotion: Sadness\n",
      "\n",
      "            Now, classify the emotion in the following statement:\n",
      "            Statement: 'That movie was so scary I had to cover my eyes.’\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "response : content='Emotion: Fear' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 102, 'total_tokens': 108}, 'model_name': 'glm-4', 'finish_reason': 'stop'} id='run--d1840633-c170-46cd-91bf-924feb957fdd-0'\n",
      "\n"
     ]
    }
   ],
   "source": [
    " #parameters  `max_new_tokens` to 10, which constrains the model to generate brief responses\n",
    "\n",
    "params = {\n",
    "    \"max_new_tokens\": 10,\n",
    "}\n",
    "\n",
    "prompt = \"\"\"Here are few examples of classifying emotions in statements:\n",
    "\n",
    "            Statement: 'I just won my first marathon!'\n",
    "            Emotion: Joy\n",
    "            \n",
    "            Statement: 'I can't believe I lost my keys again.'\n",
    "            Emotion: Frustration\n",
    "            \n",
    "            Statement: 'My best friend is moving to another country.'\n",
    "            Emotion: Sadness\n",
    "            \n",
    "            Now, classify the emotion in the following statement:\n",
    "            Statement: 'That movie was so scary I had to cover my eyes.’\n",
    "            \n",
    "\n",
    "\"\"\"\n",
    "response = llm_model(prompt, params)\n",
    "print(f\"prompt: {prompt}\\n\")\n",
    "print(f\"response : {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47bdc35-b3ad-4427-8d21-6d28d07a9453",
   "metadata": {},
   "source": [
    "The parameters are set with `max_new_tokens` to 10, which constrains the model to generate brief responses, focusing on the essential output without elaboration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3febfd69-28d8-4a23-86fe-9aa5b6f44041",
   "metadata": {},
   "source": [
    "The model's response demonstrates its ability to use the provided few examples to understand and classify the emotion of the new statement effectively following the same pattern in examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135a7b59-2725-45ea-856a-c87771634100",
   "metadata": {},
   "source": [
    "### Chain-of-thought (CoT) prompt\n",
    "\n",
    "[**Chain-of-thought (CoT) prompting**](https://www.ibm.com/think/topics/chain-of-thoughts?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-In-Context+Learning+and+Prompt+Templates-v3-GenAIcourse_1741386184) encourages the model to break down complex problems into step-by-step reasoning before arriving at a final answer. By explicitly showing or requesting intermediate steps, this technique improves the model's problem-solving abilities and reduces errors in tasks requiring multi-step reasoning. CoT is particularly effective for mathematical problems, logical reasoning, and complex decision-making tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b6fd27-edf8-4ccd-957d-1411136d2d3c",
   "metadata": {},
   "source": [
    "Here is an example of the CoT prompting technique, designed to guide the model through a sequence of reasoning steps to solve a problem. In this example, the problem is a simple arithmetic question: “A store had 22 apples. They sold 15 apples today and received a new delivery of 8 apples. How many apples are there now?”\n",
    "\n",
    "The CoT technique involves structuring the prompt by instructing the model to “Break down each step of your calculation.” This encourages the model to include explicit reasoning steps, mimicking human-like problem-solving processes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e91bae-87c3-400c-8888-9829668578b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: Consider the problem: 'A store had 22 apples. They sold 15 apples today and got a new delivery of 8 apples. \n",
      "            How many apples are there now?’\n",
      "\n",
      "            Break down each step of your calculation\n",
      "\n",
      "\n",
      "\n",
      "response : content='Certainly! To solve the problem of how many apples are in the store now after selling some and receiving a new delivery, you can follow these steps:\\n\\n1. **Initial number of apples in the store**: The problem states that there were initially 22 apples in the store.\\n\\n2. **Apples sold**: The store sold 15 apples today. To find the remaining number of apples after this sale, you need to subtract the number of apples sold from the initial number of apples.\\n\\n   Calculation for step 2:\\n   22 (initial apples) - 15 (apples sold) = 7 apples remaining\\n\\n3. **New delivery of apples**: After selling some apples, the store received a new delivery of 8 apples. To find the total number of apples in the store now, you need to add the number of apples from the new delivery to the number of remaining apples after the sale.\\n\\n   Calculation for step 3:\\n   7 (remaining apples) + 8 (new delivery) = 15 apples\\n\\nSo, after all these steps, there are now **15 apples** in the store.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 229, 'prompt_tokens': 51, 'total_tokens': 280}, 'model_name': 'glm-4', 'finish_reason': 'stop'} id='run--ca9a5ef0-6ae3-48e8-971d-6f8d7f2bff0f-0'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"temperature\": 0.5,\n",
    "}\n",
    "\n",
    "prompt = \"\"\"Consider the problem: 'A store had 22 apples. They sold 15 apples today and got a new delivery of 8 apples. \n",
    "            How many apples are there now?’\n",
    "\n",
    "            Break down each step of your calculation\n",
    "\n",
    "\"\"\"\n",
    "response = llm_model(prompt, params)\n",
    "print(f\"prompt: {prompt}\\n\")\n",
    "print(f\"response : {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba031ec-dbc6-4b8e-ac4d-55624e146e0d",
   "metadata": {},
   "source": [
    "From the response of the model, you can see the prompt directs the model to:\n",
    "\n",
    "1. Add the initial number of apples to the apples received in the new delivery.\n",
    "2. Subtract the number of apples sold from the sum obtained in the first step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd0455f-acc9-4ea0-8790-fc368d62c6bc",
   "metadata": {},
   "source": [
    "By breaking down the problem into specific steps, the model is better able to understand the sequence of operations required to arrive at the correct answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7460f69c-d488-439e-ae87-4f5dd0f26094",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "Create CoT prompts for these scenarios:\n",
    "1. Write a prompt that asks the model to think through whether a student should study tonight or go to a movie with friends, considering their upcoming test in two days.\n",
    "2. Write a prompt that instructs the model to explain the step-by-step process of making a peanut butter and jelly sandwich.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3e87ff-b685-497b-a050-3b8a5ab5be0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DECISION_MAKING RESPONSE ===\n",
      "content=\"When deciding whether to study tonight or go to a movie with friends, the student should weigh the immediate pleasure against the long-term benefits. Here's a step-by-step analysis of the pros and cons for each option:\\n\\n**Option 1: Study Tonight**\\n\\n**Pros:**\\n1. **Preparation for the Test:** The student will have more time to review and understand the material, potentially leading to a better grade on the test.\\n2. **Reduced Stress:** Getting a head start on studying can alleviate last-minute pressure and焦虑 (anxiety) before the test.\\n3. **Good Time Management:** This option demonstrates discipline and the ability to prioritize, which can be beneficial in the long run.\\n4. **No Distractions:** Studying without the temptation of social activities can lead to more focused and efficient learning.\\n\\n**Cons:**\\n1. **Social Isolation:** The student may feel left out or miss out on important social interactions with friends.\\n2. **Boredom:** Studying can be monotonous, and a break might actually improve mood and productivity in the long run.\\n3. **Immediate Satisfaction:** There is no immediate pleasure or relaxation that comes with studying, which might affect the student's mood.\\n\\n**Option 2: Go to the Movie with Friends**\\n\\n**Pros:**\\n1. **Social Interaction:** Hanging out with friends can be refreshing and provide a mental break that might actually improve mood and focus for future study sessions.\\n2. **Enjoyment:** Movies are entertaining and can help reduce stress, potentially allowing for a clearer mind when the student does decide to study.\\n3. **Balance:** Participating in social activities is an important part of a balanced life, which can contribute to overall well-being and indirectly improve academic performance.\\n\\n**Cons:**\\n1. **Less Study Time:** The student will have one less day to prepare for the test, which could lead to a less comprehensive understanding of the material.\\n2. **Potential Procrastination:** There's a risk that the student may put off studying until the last minute if not managed carefully.\\n3. **Distractions:** After the movie, the student might be tired or less focused, making it harder to start studying immediately after returning.\\n\\n**Factors to Consider:**\\n\\n1. **Importance of the Test:** If the test is worth a significant portion of the student's grade, or if it's on a difficult subject, studying may be the safer choice.\\n2. **Time Management Skills:** Does the student tend to procrastinate? If so, studying might be the wiser option to avoid the last-minute rush.\\n3. **Current Stress Levels:** If the student is already under a lot of pressure, a movie night might provide the needed relief to recharge and study more effectively later.\\n4. **Strength of Social Bonds:** If maintaining friendships is important, going to the movie might be critical for those relationships.\\n5. **Overall Well-being:** Personal health and mental state are crucial. If the student feels they can manage both their social life and studying effectively, a balanced approach might be best.\\n\\nUltimately, the decision should be based on what's most important to the student at that moment, considering the test's importance, their current stress level, and personal values. It's also important to consider whether they can reschedule the movie night to a time that doesn't conflict with the test preparation.\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 684, 'prompt_tokens': 64, 'total_tokens': 748}, 'model_name': 'glm-4', 'finish_reason': 'stop'} id='run--a665fb96-25be-4d79-b9b1-732164b4b003-0'\n",
      "\n",
      "=== SANDWICH_MAKING RESPONSE ===\n",
      "content=\"Certainly! Making a peanut butter and jelly sandwich is a classic and simple process that can be enjoyed by people of all ages. Here's a step-by-step guide to creating this timeless treat:\\n\\n**Step 1: Gathering Ingredients**\\nBefore you begin, make sure you have the following ingredients on hand:\\n\\n- 2 slices of bread (you can choose any type, such as white, whole wheat, or multigrain)\\n- Peanut butter (creamy or crunchy, according to your preference)\\n- Jelly or jam (flavors like grape, strawberry, or raspberry work well)\\n- A spreading knife or butter knife\\n- A plate or cutting board to work on\\n- Optional: butter or margarine for spreading on the bread\\n\\n**Step 2: Preparing the Workspace**\\n- Wash your hands thoroughly with soap and water.\\n- Lay out a clean plate or cutting board to prepare your sandwich on.\\n- If desired, you can also lay out a clean paper towel or napkin to place the finished sandwich on.\\n\\n**Step 3: Preparing the Bread**\\n- Take one slice of bread and place it on the plate or cutting board.\\n- If you choose to use butter or margarine, take a small amount and spread it evenly over the top of the bread slice. This step adds a richness to the sandwich and can prevent the peanut butter from soaking into the bread.\\n- Repeat the buttering process with the second slice of bread if desired.\\n\\n**Step 4: Applying the Peanut Butter**\\n- Take the jar of peanut butter and remove the lid. If there is a seal, remove that as well.\\n- Using the spreading knife, scoop out a generous amount of peanut butter.\\n- Starting from the center of the bread, spread the peanut butter evenly across the slice, reaching all the way to the edges.\\n\\n**Step 5: Adding the Jelly**\\n- Choose your preferred jelly or jam and remove the lid from the jar.\\n- With the same knife or a clean one, scoop out some jelly. The amount can vary depending on your taste, but generally, you should use a similar amount as the peanut butter.\\n- Starting from the center of the bread, spread the jelly evenly on top of the peanut butter, again reaching all the way to the edges.\\n\\n**Step 6: Assembling the Sandwich**\\n- Once both slices of bread are prepared with the peanut butter and jelly, take the unbuttered side of the second slice and place it face down on top of the first slice. The buttered or non-spread sides should now be on the outside of the sandwich.\\n\\n**Step 7: Cutting the Sandwich**\\n- If you prefer, you can cut the sandwich in half or into smaller pieces. Use a clean knife to make a clean cut straight down through the middle of the sandwich. If you're cutting it into smaller pieces, like triangles, cut each half in half again.\\n\\n**Step 8: Serving the Sandwich**\\n- Place the finished sandwich on a plate or napkin, with the cut sides facing up if it's been cut.\\n- If you have any additional snacks or sides you enjoy, like carrot sticks or a glass of milk, you can serve those alongside the sandwich.\\n\\n**Step 9: Enjoying Your Creation**\\n- With everything prepared, you can now sit down and enjoy your peanut butter and jelly sandwich. Don't forget to wash it down with your favorite drink!\\n\\nAnd there you have it – a classic peanut butter and jelly sandwich, ready to enjoy. Remember, you can adjust the ingredients and amounts to suit your personal taste.\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 724, 'prompt_tokens': 35, 'total_tokens': 759}, 'model_name': 'glm-4', 'finish_reason': 'stop'} id='run--0b145e2a-2312-4051-8edc-eaaa98c04a08-0'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Starter code: provide your solutions in the TODO parts\n",
    "\n",
    "# 1. Prompt for decision-making process\n",
    "decision_making_prompt = \"\"\"\n",
    "Consider this situation: A student is trying to decide whether to study tonight or go to a movie with friends. They have a test in two days.\n",
    "\n",
    "Think through this decision step-by-step, considering the pros and cons of each option, and what factors might be most important in making this choice.\n",
    "\"\"\"\n",
    "\n",
    "# 2. Prompt for explaining a process\n",
    "sandwich_making_prompt = \"\"\"\n",
    "Explain how to make a peanut butter and jelly sandwich.\n",
    "\n",
    "Break down each step of the process in detail, from gathering ingredients to finishing the sandwich.\n",
    "\"\"\"\n",
    "\n",
    "responses = {}\n",
    "responses[\"decision_making\"] = llm_model(decision_making_prompt)\n",
    "responses[\"sandwich_making\"] = llm_model(sandwich_making_prompt)\n",
    "\n",
    "for prompt_type, response in responses.items():\n",
    "    print(f\"=== {prompt_type.upper()} RESPONSE ===\")\n",
    "    print(response)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcc4a4a-5129-4d7f-9a86-93726894e146",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for hints</summary>\n",
    "\n",
    "```python\n",
    "# 1. Prompt for decision-making process\n",
    "decision_making_prompt = \"\"\"\n",
    "Consider this situation: A student is trying to decide whether to study tonight or go to a movie with friends. They have a test in two days.\n",
    "\n",
    "Think through this decision step-by-step, considering the pros and cons of each option, and what factors might be most important in making this choice.\n",
    "\"\"\"\n",
    "\n",
    "# 2. Prompt for explaining a process\n",
    "sandwich_making_prompt = \"\"\"\n",
    "Explain how to make a peanut butter and jelly sandwich.\n",
    "\n",
    "Break down each step of the process in detail, from gathering ingredients to finishing the sandwich.\n",
    "\"\"\"\n",
    "\n",
    "responses = {}\n",
    "responses[\"decision_making\"] = llm_model(decision_making_prompt)\n",
    "responses[\"sandwich_making\"] = llm_model(sandwich_making_prompt)\n",
    "\n",
    "for prompt_type, response in responses.items():\n",
    "    print(f\"=== {prompt_type.upper()} RESPONSE ===\")\n",
    "    print(response)\n",
    "    print()\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f028d3-3c68-4a78-b8db-60039a53fb48",
   "metadata": {},
   "source": [
    "### Self-consistency\n",
    "\n",
    "[**Self-consistency**](https://www.promptingguide.ai/techniques/consistency) is an advanced technique in which the model generates multiple independent solutions or answers to the same problem, then evaluates these different approaches to determine the most consistent or reliable result. This method enhances accuracy by leveraging the model's ability to approach problems from different angles and identify the most robust solution through comparison and verification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552ff310-f824-4bd6-b372-154340e346ee",
   "metadata": {},
   "source": [
    "This example demonstrates the self-consistency technique by reasoning through multiple calculations for a single problem. The problem posed is: “When I was 6, my sister was half my age. Now I am 70, what age is my sister?”\n",
    "\n",
    "The prompt instructs, “Provide three independent calculations and explanations, then determine the most consistent result.” This encourages the model to engage in critical thinking and consistency checking, both of which are vital for complex decision-making processes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3292c8d5-b658-42f5-bd61-8517a7e1ccfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: When I was 6, my sister was half of my age. Now I am 70, what age is my sister?\n",
      "\n",
      "            Provide three independent calculations and explanations, then determine the most consistent result.\n",
      "\n",
      "\n",
      "\n",
      "response : content=\"Let's approach this problem with three different calculations.\\n\\nCalculation 1: Direct Age Comparison\\nWhen you were 6 years old, your sister was half your age, which is 3 years younger than you. The age difference between you and your sister will always be 3 years. Now that you are 70, we can simply subtract that age difference to find your sister's current age.\\nYour sister's age = Your age - Age difference\\nYour sister's age = 70 - 3\\nYour sister's age = 67 years\\n\\nCalculation 2: Using a Ratio\\nAnother way to approach this is by using the ratio of your ages at the time you were 6. The ratio was 2:1 (you were 6, she was 3). We can maintain that ratio throughout your lives. So, for every 2 years you age, your sister ages 1 year.\\nNow, we can use this ratio to find your sister's current age:\\nYour age now / Sister's age now = Your age then / Sister's age then\\n70 / Sister's age now = 6 / 3\\nSolving for Sister's age now:\\nSister's age now = (70 * 3) / 6\\nSister's age now = 210 / 6\\nSister's age now = 35 * 6 / 6\\nSister's age now = 35 years\\nHowever, this result is clearly incorrect because it doesn't account for the fact that the ratio must be applied continuously. We must instead use the age difference of 3 years to adjust this calculation:\\nYour sister is 3 years younger, so we subtract 3 from your current age before applying the ratio:\\nAdjusted age for calculation = 70 - 3 = 67\\nNow apply the ratio:\\n67 / Sister's age now = 6 / 3\\nSister's age now = (67 * 3) / 6\\nSister's age now = 201 / 6\\nSister's age now = 33.5 * 6 / 6\\nSister's age now = 33.5 years\\nThis is still incorrect, which indicates a flaw in the application of the ratio method.\\n\\nCalculation 3: By Increment\\nWe can also calculate your sister's age by incrementing her age as you age, always maintaining the age difference of 3 years.\\nWhen you were 6, she was 3. From that point, as you age by 1 year, your sister ages by 1 year as well, but she is always 3 years behind you.\\nNow, we can simply count how many times your sister has aged by the time you reached 70:\\nYou age 1 year: Sister ages to 3 + 1 = 4\\nYou age 1 year: Sister ages to 4 + 1 = 5\\n...\\nYou age 1 year: Sister ages to 66 + 1 = 67 (on your 69th birthday)\\nYou age 1 year: Sister ages to 67 + 1 = 68 (on your 70th birthday)\\nSince the question asked for your sister's age when you are 70, your sister would be 68 years old.\\n\\nMost Consistent Result:\\nThe most consistent result is from the first and third calculations, which both yielded your sister's age as 67 years when you are 70. The second calculation was flawed due to a misunderstanding of how to apply the ratio over time. The correct answer is that your sister is 67 years old.\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 736, 'prompt_tokens': 46, 'total_tokens': 782}, 'model_name': 'glm-4', 'finish_reason': 'stop'} id='run--4b787491-6c8e-4134-93a3-e1364c7b8992-0'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_new_tokens\": 512,\n",
    "}\n",
    "\n",
    "prompt = \"\"\"When I was 6, my sister was half of my age. Now I am 70, what age is my sister?\n",
    "\n",
    "            Provide three independent calculations and explanations, then determine the most consistent result.\n",
    "\n",
    "\"\"\"\n",
    "response = llm_model(prompt, params)\n",
    "print(f\"prompt: {prompt}\\n\")\n",
    "print(f\"response : {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aae54a4-9029-4d5d-bb84-3a0264a295e6",
   "metadata": {},
   "source": [
    "The model's response demonstrates three different calculations and explanations, each using a distinct logical approach to determine the sister's age.\n",
    "\n",
    "Self-consistency can help identify the most accurate and reliable answer in scenarios where multiple plausible solutions exist.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcbd420-ce5b-4ad5-a4e5-96a82c1bfab0",
   "metadata": {},
   "source": [
    "## Applications of prompting in different use cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3106edd1-e5c4-4529-bcbe-cffe559d23e4",
   "metadata": {},
   "source": [
    "In this section, we'll demonstrate how to leverage LangChain's prompt templates to build practical applications with consistent, reproducible results. Each application follows a common pattern using the LCEL approach:\n",
    "\n",
    "1. Define the content or problem to be addressed.\n",
    "2. Create a template with variables for dynamic content.\n",
    "3. Convert the template into a LangChain PromptTemplate.\n",
    "4. Build a chain using the pipe operator `|` to connect:\n",
    "\n",
    "    - Input variables\n",
    "    - The prompt template\n",
    "    - The LLM\n",
    "    - An output parser\n",
    "\n",
    "\n",
    "5. Invoke the chain with specific inputs to generate results.\n",
    "\n",
    "This structured approach enables you to create reusable components for various NLP tasks while maintaining flexibility to adjust parameters and inputs. You'll see how this pattern applies across different use cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b804f06-f4bb-472a-8bd7-b1902b3b879a",
   "metadata": {},
   "source": [
    "### Introduction to LangChain \n",
    "\n",
    "[LangChain](https://www.langchain.com/) is a powerful framework designed to simplify the development of applications powered by language models. Built to address the challenges of working with LLMs in practical settings, LangChain provides a standardized interface for connecting models with various data sources and application environments.\n",
    "\n",
    "LangChain serves as an abstraction layer, making it easier to build complex LLM applications without handling the low-level details of model interaction. This framework has become a standard tool in the LLM ecosystem, supporting a wide range of use cases from chatbots to document analysis systems.\n",
    "\n",
    "In this section, we'll focus on LangChain's prompt template capabilities, demonstrating how they can be used to create structured, reproducible interactions with language models across different application types.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b96f4ce-d635-421e-8b1d-c5ee6418e4b6",
   "metadata": {},
   "source": [
    "### Prompt template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ee4c2d-535d-4d44-87d5-6abeca76ef69",
   "metadata": {},
   "source": [
    "[Prompt templates](https://python.langchain.com/v0.2/docs/concepts/#prompt-templates) are a key concept in LangChain. They help translate user input and parameters into instructions for a language model. These templates can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based outputs.\n",
    "\n",
    "A prompt template acts as a reusable structure for generating prompts with dynamic values. It allows you to define a consistent format while leaving placeholders for variables that change with each use case. This approach makes prompting more systematic and maintainable, especially when working with complex applications.\n",
    "\n",
    "**Modern LangChain (as of 2025) offers two main approaches to working with templates:**\n",
    "\n",
    "- The traditional `LLMChain` approach\n",
    "- The newer LangChain Expression Language (LCEL) pattern using the pipe operator `|` for more flexible composition\n",
    "\n",
    "LCEL has become the recommended pattern for building LangChain applications as it offers better composability, clearer visualization of data flow, and more flexibility when constructing complex chains.\n",
    "\n",
    "**To use a prompt template with LCEL, you typically follow these steps:**\n",
    "\n",
    "- Define your template with variables in curly braces `{}`\n",
    "- Create a `PromptTemplate` instance\n",
    "- Build a chain using the pipe operator `|` to connect components\n",
    "- Invoke the chain with your input values\n",
    "\n",
    "Let's initialize an LLM first, then demonstrate this approach. In this section, we will use the model `meta-llama/llama-3-3-70b-instruct`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277579ec-3726-480d-b06a-b93b1dc555e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"meta-llama/llama-3-3-70b-instruct\"\n",
    "model_id = \"glm-4-plus\"\n",
    "\n",
    "default_params = {\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"min_new_tokens\": 0,\n",
    "        \"temperature\": 0.5,\n",
    "        \"top_p\": 0.2,\n",
    "        \"top_k\": 1\n",
    "    }\n",
    "\n",
    "credentials = {\n",
    "        # \"url\": url,\n",
    "        \"api_key\":os.getenv('ZHIPUAI_API_KEY')\n",
    "        # uncomment the field above and replace the api_key with your actual Watsonx API key\n",
    "    }\n",
    "\n",
    "parameters = {\n",
    "    GenParams.MAX_NEW_TOKENS: 256,  # this controls the maximum number of tokens in the generated output\n",
    "    GenParams.TEMPERATURE: 0.5, # this randomness or creativity of the model's responses\n",
    "}\n",
    "\n",
    "# url = \"https://us-south.ml.cloud.ibm.com\"\n",
    "# project_id = \"skills-network\"\n",
    "\n",
    "llm = ChatZhipuAI(\n",
    "        model_id=model_id,\n",
    "        credentials=credentials,\n",
    "        params=parameters,\n",
    "        default_params=default_params\n",
    "    )\n",
    "# llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba49348-f035-4d25-ac93-b87c775340b8",
   "metadata": {},
   "source": [
    "Use the `PromptTemplate` to create a template for a string-based prompt. In this template, you'll define two parameters: `adjective` and `content`. These parameters allow for the reuse of the prompt across different situations. For instance, to adapt the prompt to various contexts, simply pass the relevant values to these parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9856b0b-f248-411a-a516-a42df5fbcad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['adjective', 'content'], input_types={}, partial_variables={}, template='Tell me a {adjective} joke about {content}.\\n')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Tell me a {adjective} joke about {content}.\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35aa788-831d-4777-8dae-3689ca09a56b",
   "metadata": {},
   "source": [
    "Now, let's take a look at how the prompt has been formatted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bc955e-282e-4132-9d0e-35c9b82ea018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a funny joke about chickens.\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(adjective=\"funny\", content=\"chickens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ecae78-9e72-45cf-96df-95e708ea9377",
   "metadata": {},
   "source": [
    "From the response, you can see that the prompt is formatted according to the specified context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a4382d-621a-4a5f-baaf-6f1979080423",
   "metadata": {},
   "source": [
    "To ensure consistent formatting of the prompts, we will define a helper function `format_prompt`. This function takes a dictionary of variables and applies them to our prompt template. It ensures that all placeholder variables (like {adjective} and {content}) are properly replaced with their values before the prompt is sent to the language model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd4f40d-a3c1-44c6-8799-3a1b8ca36b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Define a function to ensure proper formatting\n",
    "def format_prompt(variables):\n",
    "    return prompt.format(**variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819f295a-bc8b-4000-8102-da01c86a59b7",
   "metadata": {},
   "source": [
    "The following code builds a chain using the LCEL (LangChain Expression Language) pattern. This chain connects components using the pipe operator (`|`) to create a processing flow. The chain takes input variables, passes them through the prompt template, sends the formatted prompt to the LLM, and uses a string output parser to return the final response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d540d104-11f3-4fb1-8d86-f2609c6dc0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the chicken cross the playground?\n",
      "\n",
      "To get to the other slide!\n"
     ]
    }
   ],
   "source": [
    "# Create the chain with explicit formatting\n",
    "joke_chain = (\n",
    "    RunnableLambda(format_prompt)\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Run the chain\n",
    "response = joke_chain.invoke({\"adjective\": \"funny\", \"content\": \"chickens\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b519c47-d726-4249-9b77-5eeda0831338",
   "metadata": {},
   "source": [
    "From the response, you can see the LLM came up with a funny joke about chickens.\n",
    "\n",
    "To use this prompt in another context, simply replace the variables accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a6cba3-f5e9-4147-a212-c14652bda82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the goldfish win the award for being the saddest fish?\n",
      "\n",
      "Because it always had a \"fish-ion\" to cry.\n"
     ]
    }
   ],
   "source": [
    "response = joke_chain.invoke({\"adjective\": \"sad\", \"content\": \"fish\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8e3b93-6700-4e97-9e50-d2c6a5b8abba",
   "metadata": {},
   "source": [
    "In the following sections, you will learn how to create agents capable of completing various tasks using prompt templates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a63331-4e8c-48bc-9646-d911d6969c40",
   "metadata": {},
   "source": [
    "### Text summarization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aed4faa-2782-456f-aaa4-9746cf39f1a6",
   "metadata": {},
   "source": [
    "Here is a text summarization agent designed to help summarize the content you provide to the LLM. The LCEL chain takes your content as input, processes it through the prompt template, sends it to the language model, and returns a concise summary.\n",
    "\n",
    "You can store the content to be summarized in a variable, allowing for repeated use with different texts:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a68081-8a95-4f3a-8ad0-69f81b628093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rapid growth of technology, including AI and IoT, has revolutionized industries like healthcare and education, improving diagnostics, transportation efficiency, and global educational access, fostering a more productive and connected society.\n"
     ]
    }
   ],
   "source": [
    "content = \"\"\"\n",
    "    The rapid advancement of technology in the 21st century has transformed various industries, including healthcare, education, and transportation. \n",
    "    Innovations such as artificial intelligence, machine learning, and the Internet of Things have revolutionized how we approach everyday tasks and complex problems. \n",
    "    For instance, AI-powered diagnostic tools are improving the accuracy and speed of medical diagnoses, while smart transportation systems are making cities more efficient and reducing traffic congestion. \n",
    "    Moreover, online learning platforms are making education more accessible to people around the world, breaking down geographical and financial barriers. \n",
    "    These technological developments are not only enhancing productivity but also contributing to a more interconnected and informed society.\n",
    "\"\"\"\n",
    "\n",
    "template = \"\"\"Summarize the {content} in one sentence.\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Create the LCEL chain\n",
    "summarize_chain = (\n",
    "    RunnableLambda(format_prompt)\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Run the chain\n",
    "summary = summarize_chain.invoke({\"content\": content})\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c392cc55-1ec1-41a4-bc5e-147ec0862b20",
   "metadata": {},
   "source": [
    "### Question answering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de81b61-11fc-4e1d-aad9-5d4e4cd681ad",
   "metadata": {},
   "source": [
    "Here is a Q&A agent built using the LCEL pattern.\n",
    "\n",
    "This agent enables the LLM to learn from the provided content and answer questions based on what it has learned. Occasionally, if the LLM does not have sufficient information, it may generate a speculative answer. To manage this, we'll specifically instruct it to respond with \"Unsure about the answer\" if it is uncertain about the correct response.\n",
    "\n",
    "The chain takes both the content (context) and question as inputs, processing them through our template before sending them to the LLM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4017b01e-068a-49e9-918c-92c23aa661d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mercury, Venus, Earth, and Mars.\n"
     ]
    }
   ],
   "source": [
    "content = \"\"\"\n",
    "    The solar system consists of the Sun, eight planets, their moons, dwarf planets, and smaller objects like asteroids and comets. \n",
    "    The inner planets—Mercury, Venus, Earth, and Mars—are rocky and solid. \n",
    "    The outer planets—Jupiter, Saturn, Uranus, and Neptune—are much larger and gaseous.\n",
    "\"\"\"\n",
    "\n",
    "question = \"Which planets in the solar system are rocky and solid?\"\n",
    "\n",
    "template = \"\"\"\n",
    "    Answer the {question} based on the {content}.\n",
    "    Respond \"Unsure about answer\" if not sure about the answer.\n",
    "    \n",
    "    Answer:\n",
    "    \n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Create the LCEL chain\n",
    "qa_chain = (\n",
    "    RunnableLambda(format_prompt)\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Run the chain\n",
    "answer = qa_chain.invoke({\"question\": question, \"content\": content})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478bb553-3e74-4eb9-bfc9-eac0fe6fe8cd",
   "metadata": {},
   "source": [
    "### Text classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c722940-c863-4fa9-92a0-eb7af8e0c37a",
   "metadata": {},
   "source": [
    "Here is a text classification agent designed to categorize text into predefined categories. This example employs zero-shot learning, where the agent classifies text without prior exposure to related examples.\n",
    "\n",
    "Using the LCEL approach, we create a chain that takes both the text to be classified and the available categories as inputs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d266125-6d02-419c-a72a-7dd722195604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: Music.\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "    The concert last night was an exhilarating experience with outstanding performances by all artists.\n",
    "\"\"\"\n",
    "\n",
    "categories = \"Entertainment, Food and Dining, Technology, Literature, Music.\"\n",
    "\n",
    "template = \"\"\"\n",
    "    Classify the {text} into one of the {categories}.\n",
    "    \n",
    "    Category:\n",
    "    \n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Create the LCEL chain\n",
    "classification_chain = (\n",
    "    RunnableLambda(format_prompt)\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Run the chain\n",
    "category = classification_chain.invoke({\"text\": text, \"categories\": categories})\n",
    "print(category)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b99560-25d2-407c-ac2e-51a019383661",
   "metadata": {},
   "source": [
    "### Code generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a6d4ae-d220-4c40-a164-2853d41c667b",
   "metadata": {},
   "source": [
    "Here is an example of an SQL code generation agent built with LCEL. This agent is designed to generate SQL queries based on provided descriptions. It interprets the requirements from your input and translates them into executable SQL code.\n",
    "\n",
    "The chain takes your natural language description and transforms it into a properly formatted SQL query:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d1bf12-6719-4504-a69a-a850c78b8509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To achieve this, you would need to join the 'customers' table with the 'purchases' table on a common field, such as a 'customer_id'. Then, you would filter the results to include only those customers who have a purchase_date within the last 30 days.\n",
      "\n",
      "Here is an SQL query that would do this:\n",
      "\n",
      "```sql\n",
      "SELECT DISTINCT c.customer_name, c.email\n",
      "FROM customers c\n",
      "JOIN purchases p ON c.customer_id = p.customer_id\n",
      "WHERE p.purchase_date > CURRENT_DATE - INTERVAL '30 days';\n",
      "```\n",
      "\n",
      "This query does the following:\n",
      "- It selects unique combinations of the 'customer_name' and 'email' fields from the 'customers' table, avoiding any duplicates through the use of the `DISTINCT` keyword.\n",
      "- It joins the 'customers' table (`c`) with the 'purchases' table (`p`) on the 'customer_id' field.\n",
      "- It filters the results to only include purchases made in the last 30 days using the `WHERE` clause and the `CURRENT_DATE - INTERVAL '30 days'` to calculate the date 30 days ago from the current date.\n",
      "\n",
      "Please note that the exact SQL syntax can vary depending on the database management system you are using (MySQL, PostgreSQL, SQL Server, etc.). The above query is written in standard SQL and should work on most systems with minor adjustments.\n"
     ]
    }
   ],
   "source": [
    "description = \"\"\"\n",
    "    Retrieve the names and email addresses of all customers from the 'customers' table who have made a purchase in the last 30 days. \n",
    "    The table 'purchases' contains a column 'purchase_date'\n",
    "\"\"\"\n",
    "\n",
    "template = \"\"\"\n",
    "    Generate an SQL query based on the {description}\n",
    "    \n",
    "    SQL Query:\n",
    "    \n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Create the LCEL chain\n",
    "sql_generation_chain = (\n",
    "    RunnableLambda(format_prompt) \n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Run the chain\n",
    "sql_query = sql_generation_chain.invoke({\"description\": description})\n",
    "print(sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7483f4fc-f6e9-4847-9b9c-51b4b569a1d5",
   "metadata": {},
   "source": [
    "### Role playing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e59573d-e131-4516-89c6-a7fd719774a2",
   "metadata": {},
   "source": [
    "You can also configure the LLM to assume specific roles as defined by us, enabling it to follow predetermined rules and behave like a task-oriented chatbot.\n",
    "\n",
    "This approach separates the role definition from the prompt structure, allowing for easy role-switching without rewriting the entire prompt. The key components are:\n",
    "\n",
    "- `role`: Specifies the character, expertise, or persona the LLM should embody\n",
    "- `tone`: Defines the communication style and emotional quality of responses\n",
    "- `question`: Contains the user's query that needs addressing\n",
    "\n",
    "By parameterizing these elements, you can rapidly change the LLM's behavior by adjusting single variables rather than rewriting entire prompts. This pattern is particularly valuable for building conversational agents that need to serve different functions or adapt to various contexts.\n",
    "\n",
    "For example, the code below configures the LLM to act as a game master. In this role, the LLM answers questions about games while maintaining an engaging and immersive tone, enhancing the user experience. You can test the bot by asking questions related to tabletop role-playing games or game mastering. Try asking about game rules, storytelling techniques, player management, or setting descriptions such as:\n",
    "\n",
    "1. \"Who are you?\"\n",
    "2. \"What are the basic rules of Dungeons & Dragons?\"\n",
    "3. \"How do I create a balanced encounter for my players?\"\n",
    "4. \"Can you describe a mysterious forest setting for my adventure?\"\n",
    "5. \"What's a good puzzle I could use in my dungeon?\"\n",
    "6. \"How do I handle a player who is constantly interrupting others?\"\n",
    "\n",
    "The function is written within a while loop, allowing continuous interaction. **To exit the loop and terminate the conversation, type \"quit,\" \"exit,\" or \"bye\" into the input box.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1165e1-19ec-4d9f-a9c5-0cf6da4a7ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = \"\"\"\n",
    "    Dungeon & Dragons game master\n",
    "\"\"\"\n",
    "\n",
    "tone = \"engaging and immersive\"\n",
    "\n",
    "template = \"\"\"\n",
    "    You are an expert {role}. I have this question {question}. I would like our conversation to be {tone}.\n",
    "    \n",
    "    Answer:\n",
    "    \n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Create the LCEL chain\n",
    "roleplay_chain = (\n",
    "    RunnableLambda(format_prompt)\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Create an interactive chat loop\n",
    "while True:\n",
    "    query = input(\"Question: \")\n",
    "    \n",
    "    if query.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
    "        print(\"Answer: Goodbye!\")\n",
    "        break\n",
    "        \n",
    "    response = roleplay_chain.invoke({\"role\": role, \"question\": query, \"tone\": tone})\n",
    "    print(\"Answer: \", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f25d3c-a52f-4739-9a65-1f1a7b2c4e14",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "**Create an LCEL Chain with Custom Formatting**\n",
    "\n",
    "In this exercise, you'll create your own LCEL chain that uses prompt templates to build a custom application.\n",
    "\n",
    "**Task:** Create a product review analyzer that can:\n",
    "1. Identify the sentiment (positive, negative, or neutral).\n",
    "2. Extract mentioned product features.\n",
    "3. Provide a one-sentence summary of the review.\n",
    "\n",
    "**Steps:**\n",
    "1. Create a prompt template with placeholders for the review text.\n",
    "2. Build an LCEL chain that formats your prompt properly.\n",
    "3. Process the sample reviews and display the results.\n",
    "4. Try modifying the chain to change the output format.\n",
    "\n",
    "**Sample input:**\n",
    "```python\n",
    "reviews = [\n",
    "    \"I love this smartphone! The camera quality is exceptional and the battery lasts all day. The only downside is that it heats up a bit during gaming.\",\n",
    "    \"This laptop is terrible. It's slow, crashes frequently, and the keyboard stopped working after just two months. Customer service was unhelpful.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35283a7f-b9d9-4128-a0b4-b6f07b9bd8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Starter code: provide your solutions in the TODO parts\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# model_id = \"meta-llama/llama-3-3-70b-instruct\"\n",
    "model_id = \"glm-4-plus\"\n",
    "\n",
    "credentials = {\n",
    "        # \"url\": url,\n",
    "        \"api_key\":os.getenv('ZHIPUAI_API_KEY')\n",
    "        # uncomment the field above and replace the api_key with your actual Watsonx API key\n",
    "    }\n",
    "\n",
    "parameters = {\n",
    "    GenParams.MAX_NEW_TOKENS: 512,  # this controls the maximum number of tokens in the generated output\n",
    "    GenParams.TEMPERATURE: 0.2, # this randomness or creativity of the model's responses\n",
    "}\n",
    "\n",
    "# url = \"https://us-south.ml.cloud.ibm.com\"\n",
    "# project_id = \"skills-network\"\n",
    "\n",
    "llm = ChatZhipuAI(\n",
    "        model_id=model_id,\n",
    "        credentials=credentials,\n",
    "        params=parameters\n",
    "    )\n",
    "\n",
    "# Create the prompt template\n",
    "template = \"\"\"\n",
    "Analyze the following product review:\n",
    "\"{review}\"\n",
    "\n",
    "Provide your analysis in the following format:\n",
    "- Sentiment: (positive, negative, or neutral)\n",
    "- Key Features Mentioned: (list the product features mentioned)\n",
    "- Summary: (one-sentence summary)\n",
    "\"\"\"\n",
    "\n",
    "product_review_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Create a formatting function\n",
    "def format_review_prompt(variables):\n",
    "    return product_review_prompt.format(**variables)\n",
    "\n",
    "# Build the LCEL chain\n",
    "review_analysis_chain = (\n",
    "    RunnableLambda(format_review_prompt)\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Process the reviews\n",
    "reviews = [\n",
    "    \"I love this smartphone! The camera quality is exceptional and the battery lasts all day. The only downside is that it heats up a bit during gaming.\",\n",
    "    \"This laptop is terrible. It's slow, crashes frequently, and the keyboard stopped working after just two months. Customer service was unhelpful.\"\n",
    "]\n",
    "\n",
    "for i, review in enumerate(reviews):\n",
    "    print(f\"==== Review #{i+1} ====\")\n",
    "    result = review_analysis_chain.invoke({\"review\": review})\n",
    "    print(result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e217aad-7303-4361-a66d-497f13348562",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for hints</summary>\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "model_id = \"meta-llama/llama-3-3-70b-instruct\"\n",
    "\n",
    "parameters = {\n",
    "    GenParams.MAX_NEW_TOKENS: 512,  # this controls the maximum number of tokens in the generated output\n",
    "    GenParams.TEMPERATURE: 0.2, # this randomness or creativity of the model's responses\n",
    "}\n",
    "\n",
    "url = \"https://us-south.ml.cloud.ibm.com\"\n",
    "project_id = \"skills-network\"\n",
    "\n",
    "llm = WatsonxLLM(\n",
    "        model_id=model_id,\n",
    "        url=url,\n",
    "        project_id=project_id,\n",
    "        params=parameters\n",
    "    )\n",
    "\n",
    "# Create the prompt template\n",
    "template = \"\"\"\n",
    "Analyze the following product review:\n",
    "\"{review}\"\n",
    "\n",
    "Provide your analysis in the following format:\n",
    "- Sentiment: (positive, negative, or neutral)\n",
    "- Key Features Mentioned: (list the product features mentioned)\n",
    "- Summary: (one-sentence summary)\n",
    "\"\"\"\n",
    "\n",
    "product_review_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Create a formatting function\n",
    "def format_review_prompt(variables):\n",
    "    return product_review_prompt.format(**variables)\n",
    "\n",
    "# Build the LCEL chain\n",
    "review_analysis_chain = (\n",
    "    RunnableLambda(format_review_prompt)\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Process the reviews\n",
    "reviews = [\n",
    "    \"I love this smartphone! The camera quality is exceptional and the battery lasts all day. The only downside is that it heats up a bit during gaming.\",\n",
    "    \"This laptop is terrible. It's slow, crashes frequently, and the keyboard stopped working after just two months. Customer service was unhelpful.\"\n",
    "]\n",
    "\n",
    "for i, review in enumerate(reviews):\n",
    "    print(f\"==== Review #{i+1} ====\")\n",
    "    result = review_analysis_chain.invoke({\"review\": review})\n",
    "    print(result)\n",
    "    print()\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16301e51-e328-414d-84b9-dfe201f300fc",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations on completing this lab on prompt engineering and LangChain prompt templates! You've successfully navigated from basic prompting techniques to more advanced approaches including zero-shot, one-shot, few-shot learning, as well as chain-of-thought reasoning and self-consistency prompting. You then applied these concepts practically using LangChain's prompt templates with the modern LCEL pattern to create various applications.\n",
    "\n",
    "By learning how to properly structure prompts and build composable chains with the pipe operator (`|`), you've gained essential skills for developing robust LLM applications. These techniques provide a solid foundation for creating more complex systems and getting the most out of any language model you work with.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d12c5c-2a30-40b6-85b6-3503d7b15656",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d12ac6-c933-43a1-8fc2-ccbb7fe979dc",
   "metadata": {},
   "source": [
    "[Hailey Quach](https://www.haileyq.com) is a Data Scientist at IBM.\n",
    "\n",
    "[Kang Wang](https://author.skills.network/instructors/kang_wang) is a Data Scientist at IBM. He is also a PhD Candidate in the University of Waterloo.\n",
    "\n",
    "[Faranak Heidari](https://author.skills.network/instructors/faranak_heidari) is a Data Scientist at IBM. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4fe408-4cb6-4aaf-b238-f5119c703822",
   "metadata": {},
   "source": [
    "<!---\n",
    "## Change log\n",
    "\n",
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2025-03-07|1.1|Hailey Quach|Updated lab|\n",
    "|2025-04-03|1.2|Jojy John|ID Reviewed|\n",
    "|2025-04-03|1.2|Rahul Rawat|QA pass|\n",
    "--->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9531ea7a-3ae1-417e-907a-b0c177bb8838",
   "metadata": {},
   "source": [
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IBMai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "prev_pub_hash": "72564ca54856372110316bd2ac886959f6a240bc228a6c53e01c95828e5bb957"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
